{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Preprocessing\n",
    "* CRF\n",
    "* Comparing Performance on Clean & Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jacobsw/Desktop/WORK/OJO_CODE/HOMECITY_CRF/\"\n",
    "filename = \"20160812-tagged-conversations-training.jsons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_homecity(path, filename):\n",
    "    \n",
    "    data = []\n",
    "    for line in open(path+filename,'r'):\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "    return data\n",
    "\n",
    "def filter_by_received(data):\n",
    "    \n",
    "    entries = []\n",
    "    for datum in data:\n",
    "        for entry in datum['conversation']:\n",
    "            if entry['received']:\n",
    "                entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "r = re.compile('\\S+')\n",
    "def find_words(s, start, end):\n",
    "    results = []\n",
    "    for j,m in enumerate(r.finditer(s)):\n",
    "        if m.end() < start:\n",
    "            continue\n",
    "        elif m.start() < end:\n",
    "            results.append(j)\n",
    "            # results.append((j, m.group(), m.start(), m.end()))\n",
    "        else:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "def process_untagged(entry): \n",
    "    \n",
    "    words = entry['body'].split()\n",
    "    labels = ['O'] * len(words)\n",
    "    \n",
    "    return words, labels\n",
    "\n",
    "def process_tagged(entry):\n",
    "    \n",
    "    sent = entry['body']\n",
    "    \n",
    "    words = sent.split()\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    for entity in entry['entity_sets'][0]['entities']:\n",
    "        w_idxs = find_words(sent, entity['starting_char'], entity['ending_char'])\n",
    "        labels[w_idxs[0]] = 'B-' + entity['entity']\n",
    "        if len(w_idxs)>1:\n",
    "            for i in xrange(1,len(w_idxs)):\n",
    "                labels[w_idxs[i]] = 'I-' + entity['entity']\n",
    "    \n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load_homecity(data_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entries = filter_by_received(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_crf_format(entries):\n",
    "    \n",
    "    words_list, labels_list = [], []\n",
    "    for i,entry in enumerate(entries):\n",
    "        try:\n",
    "            if len(entry['entity_sets'])==0:\n",
    "                words, labels = process_untagged(entry)\n",
    "            else:\n",
    "                words, labels = process_tagged(entry)\n",
    "            words_list.append(words)\n",
    "            labels_list.append(labels)\n",
    "        except: # if there's exception, would be captured.\n",
    "            print i\n",
    "    \n",
    "    return words_list, labels_list\n",
    "\n",
    "def all_o(labels):\n",
    "    return all(lb=='O' for lb in labels)\n",
    "\n",
    "def non_all_o_subset(X, Y):\n",
    "    \n",
    "    X_sub, Y_sub = [], []\n",
    "    for i in xrange(len(X)):\n",
    "        if not all_o(Y[i]):\n",
    "            X_sub.append(X[i])\n",
    "            Y_sub.append(Y[i])\n",
    "    \n",
    "    return X_sub, Y_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = non_all_o_subset(*to_crf_format(entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "def merge(x, y):\n",
    "\n",
    "    x2y = []\n",
    "    curr_x = 0\n",
    "    curr_y = 0\n",
    "    len_x = 0\n",
    "    len_y = 0\n",
    "    for j, xx in enumerate(x):\n",
    "        while len_y < len_x:\n",
    "            len_y += len(y[curr_y])\n",
    "            curr_y += 1\n",
    "        x2y.append(curr_y)\n",
    "        len_x += len(x[curr_x])\n",
    "        curr_x += 1\n",
    "        \n",
    "    return x2y\n",
    "\n",
    "def filter_extra(x2y, fts):\n",
    "    return [fts[i] for i in x2y]\n",
    "\n",
    "def extract_info(words): \n",
    "    # assuming parser = spacy.English()\n",
    "    \n",
    "    if type(words)==list: sent = ' '.join(words)\n",
    "        \n",
    "    parsed = nlp(unicode(sent))# if type(sent)==str else parser(unicode(sent))\n",
    "    tokens = [token.orth_ for token in parsed]    \n",
    "    x2y = merge(words, tokens)\n",
    "    \n",
    "    pos = [token.pos_ for token in parsed]\n",
    "    ner = ['none' if token.ent_type_=='' else token.ent_type_ for token in parsed]\n",
    "    dep_rel = [token.dep_ for token in parsed]\n",
    "    dep_head = [token.head.orth_ for token in parsed]\n",
    "    \n",
    "    return filter_extra(x2y,pos), \\\n",
    "           filter_extra(x2y,ner), \\\n",
    "           filter_extra(x2y,dep_rel), \\\n",
    "           filter_extra(x2y,dep_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_aug = [extract_info(X_i) for X_i in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(words, labels):\n",
    "    \n",
    "    len_sent = len(words)\n",
    "    all_fts = []\n",
    "\n",
    "    for i in xrange(len_sent):\n",
    "        fts = [words[i]]\n",
    "        if i > 0:\n",
    "            fts += ['-1-'+words[i-1],'-1-L-'+labels[i-1]]\n",
    "        else: fts += ['BOS2'] # BOS2, BOS1, first_word, ...\n",
    "        if i > 1:\n",
    "            fts += ['-2-'+words[i-2],'-2-L-'+labels[i-2]]\n",
    "        else: fts += ['BOS1']\n",
    "        if i < len_sent - 1:\n",
    "            fts += ['+1-'+words[i+1],'+1-L-'+labels[i+1]]\n",
    "        else: fts += ['EOS2'] # last_word, EOS1, EOS2\n",
    "        if i < len_sent - 2:\n",
    "            fts += ['+2-'+words[i+2],'+2-L-'+labels[i+2]]\n",
    "        else: fts += ['EOS1']\n",
    "        all_fts.append(fts)\n",
    "    \n",
    "    return all_fts    \n",
    "\n",
    "def featurize_aug(words, augs, labels):\n",
    "    \n",
    "    len_sent = len(words)\n",
    "    all_fts = []\n",
    "    pos, ner, dep_rel, dep_head = augs\n",
    "    for i in xrange(len_sent):\n",
    "        fts = [words[i],pos[i],ner[i],dep_rel[i],dep_head[i]]\n",
    "        if i > 0:\n",
    "            fts += ['-1-'+words[i-1],'-1-L-'+labels[i-1],\n",
    "                    '-1-POS'+pos[i-1],'-1-NER'+ner[i-1],'-1-DR'+dep_rel[i-1],'-1-DH'+dep_head[i-1]]\n",
    "        else: fts += ['BOS2'] # BOS2, BOS1, first_word, ...\n",
    "        if i > 1:\n",
    "            fts += ['-2-'+words[i-2],'-2-L-'+labels[i-2],\n",
    "                    '-2-POS'+pos[i-2],'-2-NER'+ner[i-2],'-2-DR'+dep_rel[i-2],'-2-DH'+dep_head[i-2]]\n",
    "        else: fts += ['BOS1']\n",
    "        if i < len_sent - 1:\n",
    "            fts += ['+1-'+words[i+1],'+1-L-'+labels[i+1],\n",
    "                    '+1-POS'+pos[i+1],'+1-NER'+ner[i+1],'+1-DR'+dep_rel[i+1],'+1-DH'+dep_head[i+1]]\n",
    "        else: fts += ['EOS2'] # last_word, EOS1, EOS2\n",
    "        if i < len_sent - 2:\n",
    "            fts += ['+2-'+words[i+2],'+2-L-'+labels[i+2],\n",
    "                    '+2-POS'+pos[i+2],'+2-NER'+ner[i+2],'+2-DR'+dep_rel[i+2],'+2-DH'+dep_head[i+2]]\n",
    "        else: fts += ['EOS1']\n",
    "        all_fts.append(fts)\n",
    "    \n",
    "    return all_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_train_aug, X_test_aug, Y_train, Y_test = train_test_split(X, X_aug, Y, test_size=.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_fts = [featurize(words,labels) for words,labels in zip(X_train,Y_train)]\n",
    "X_test_fts = [featurize(words,labels) for words,labels in zip(X_test,Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_aug_fts = [featurize_aug(words,augs,labels) for words,augs,labels in zip(X_train,X_train_aug,Y_train)]\n",
    "X_test_aug_fts = [featurize_aug(words,augs,labels) for words,augs,labels in zip(X_test,X_test_aug,Y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CRF(object):\n",
    "    \n",
    "    def run(self, X_train_fts, X_test_fts, Y_train, Y_test):\n",
    "\n",
    "        print \"... Training\"\n",
    "        crf = pycrfsuite.Trainer(verbose=0)\n",
    "        for x,y in zip(X_train_fts, Y_train):\n",
    "            crf.append(x,y)\n",
    "        crf.set_params({'c1':1.,'c2':1e-3,'max_iterations':100,'feature.possible_transitions':True})\n",
    "        crf.train('crf_homecity.crfsuite')\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open('crf_homecity.crfsuite')\n",
    "\n",
    "        print \"... Evaluating\"\n",
    "        y_true = Y_test\n",
    "        y_pred = [tagger.tag(fts) for fts in X_test_fts]\n",
    "        lb = LabelBinarizer()\n",
    "        y_true_in_tags = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "        y_pred_in_tags = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        O_idx = lb.transform(['O']).argmax()\n",
    "        class_indices = {cls:idx for idx,cls in enumerate(lb.classes_)}\n",
    "        print classification_report(\n",
    "            y_true_in_tags[y_true_in_tags.argmax(1) != O_idx],\n",
    "            y_pred_in_tags[y_true_in_tags.argmax(1) != O_idx],\n",
    "            labels = [class_indices[cls] for cls in lb.classes_],\n",
    "            target_names = lb.classes_\n",
    "        )\n",
    "        print\n",
    "        print \"Accuracy: %.6f%%\" % (accuracy_score(y_true_in_tags, y_pred_in_tags)*100)\n",
    "        self.y_t = y_true_in_tags\n",
    "        self.y_p = y_pred_in_tags\n",
    "        self.lb = lb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Comparing Performance on Clean & Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crf = CRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Training\n",
      "... Evaluating\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   B-ojo.status       1.00      0.33      0.50        75\n",
      "      B-property_search.amenity       1.00      0.63      0.77        43\n",
      "    B-property_search.bathrooms       1.00      0.50      0.67        16\n",
      "     B-property_search.bedrooms       1.00      0.74      0.85        19\n",
      "        B-property_search.floor       0.00      0.00      0.00         1\n",
      "     B-property_search.location       0.91      0.52      0.66       149\n",
      "     B-property_search.lot_size       1.00      0.57      0.73         7\n",
      "        B-property_search.price       1.00      0.16      0.28        37\n",
      "B-property_search.property_type       1.00      0.52      0.68        27\n",
      "         B-property_search.sqft       1.00      0.20      0.33        10\n",
      "  B-property_search.unit_floors       0.00      0.00      0.00         1\n",
      "   B-property_search.year_built       0.00      0.00      0.00         1\n",
      "                   I-ojo.status       1.00      0.97      0.98       260\n",
      "      I-property_search.amenity       0.92      0.88      0.90        26\n",
      "    I-property_search.bathrooms       1.00      1.00      1.00         2\n",
      "     I-property_search.bedrooms       1.00      1.00      1.00         2\n",
      "     I-property_search.location       0.83      0.82      0.82        87\n",
      "     I-property_search.lot_size       1.00      1.00      1.00         4\n",
      "        I-property_search.price       0.00      0.00      0.00         3\n",
      "I-property_search.property_type       1.00      1.00      1.00        10\n",
      "                              O       0.00      0.00      0.00         0\n",
      "\n",
      "                    avg / total       0.95      0.69      0.77       780\n",
      "\n",
      "\n",
      "Accuracy: 97.376228%\n",
      "CPU times: user 24.4 s, sys: 102 ms, total: 24.5 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf.run(X_train_fts, X_test_fts, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Training\n",
      "... Evaluating\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   B-ojo.status       1.00      0.33      0.50        75\n",
      "      B-property_search.amenity       0.93      0.63      0.75        43\n",
      "    B-property_search.bathrooms       1.00      0.69      0.81        16\n",
      "     B-property_search.bedrooms       1.00      0.79      0.88        19\n",
      "        B-property_search.floor       0.00      0.00      0.00         1\n",
      "     B-property_search.location       0.92      0.60      0.72       149\n",
      "     B-property_search.lot_size       1.00      0.57      0.73         7\n",
      "        B-property_search.price       1.00      0.65      0.79        37\n",
      "B-property_search.property_type       1.00      0.52      0.68        27\n",
      "         B-property_search.sqft       1.00      0.20      0.33        10\n",
      "  B-property_search.unit_floors       0.00      0.00      0.00         1\n",
      "   B-property_search.year_built       0.00      0.00      0.00         1\n",
      "                   I-ojo.status       1.00      0.97      0.98       260\n",
      "      I-property_search.amenity       1.00      0.88      0.94        26\n",
      "    I-property_search.bathrooms       1.00      0.50      0.67         2\n",
      "     I-property_search.bedrooms       1.00      1.00      1.00         2\n",
      "     I-property_search.location       0.88      0.86      0.87        87\n",
      "     I-property_search.lot_size       1.00      1.00      1.00         4\n",
      "        I-property_search.price       0.00      0.00      0.00         3\n",
      "I-property_search.property_type       1.00      0.80      0.89        10\n",
      "                              O       0.00      0.00      0.00         0\n",
      "\n",
      "                    avg / total       0.96      0.74      0.82       780\n",
      "\n",
      "\n",
      "Accuracy: 97.700723%\n",
      "CPU times: user 37.5 s, sys: 111 ms, total: 37.6 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf.run(X_train_aug_fts, X_test_aug_fts, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
